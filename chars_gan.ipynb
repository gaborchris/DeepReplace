{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chars_gan",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaborchris/DeepReplace/blob/master/chars_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOuYg9rkmc1b",
        "colab_type": "code",
        "outputId": "ddabdd79-b1c8-487b-f68c-530adc2d5ccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "try:\n",
        "  from google.colab import files\n",
        "except Exception:\n",
        "  print(Exception)\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "import pathlib"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF40fEZi4hb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "NOISE_DIM = 100\n",
        "\n",
        "num_to_char = {}\n",
        "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
        "for i in range(0, 10):\n",
        "    num_to_char[i] = str(i)\n",
        "for i in range(0, 26):\n",
        "    num_to_char[i+10] = alphabet[i].upper()\n",
        "for i in range(0, 26):\n",
        "    num_to_char[i+10+26] = alphabet[i].lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAS3PZw_4pIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_generator_model(n_classes=10):\n",
        "    # Create class embedding channel\n",
        "    input_label = tf.keras.layers.Input(shape=(1,))\n",
        "    label_embedding = tf.keras.layers.Embedding(n_classes, 50)(input_label)\n",
        "    upscaling = tf.keras.layers.Dense(7*7*1)(label_embedding)\n",
        "    upscaling = tf.keras.layers.Reshape((7, 7, 1))(upscaling)\n",
        "\n",
        "    # create seed encoding network\n",
        "    seed_input = tf.keras.layers.Input(shape=(NOISE_DIM,))\n",
        "    seed_fc = tf.keras.layers.Dense(7*7*256, use_bias=False)(seed_input)\n",
        "    seed_fc = tf.keras.layers.BatchNormalization()(seed_fc)\n",
        "    seed_fc = tf.keras.layers.LeakyReLU()(seed_fc)\n",
        "    seed_fc = tf.keras.layers.Reshape((7, 7, 256))(seed_fc)\n",
        "\n",
        "    # merge embedding with seed encoder\n",
        "    merge = tf.keras.layers.Concatenate()([seed_fc, upscaling])\n",
        "    assert tuple(merge.shape) == (None, 7, 7, 256+1)\n",
        "\n",
        "    x = layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False)(merge)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    assert tuple(x.shape) == (None, 7, 7, 128)\n",
        "\n",
        "    x = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    assert tuple(x.shape) == (None, 14, 14, 64)\n",
        "\n",
        "    # TODO tanh function with output between [-1, 1]\n",
        "    output = layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')(x)\n",
        "    model = tf.keras.Model([seed_input, input_label], output)\n",
        "    assert model.output_shape == (None, 28, 28, 3)\n",
        "\n",
        "    return model\n",
        "\n",
        "def make_discriminator_model(n_classes=10):\n",
        "    # Create a class embedding\n",
        "    input_label = tf.keras.layers.Input(shape=(1,))\n",
        "    label_embedding = tf.keras.layers.Embedding(n_classes, 50)(input_label)\n",
        "    upscaling = tf.keras.layers.Dense(28*28)(label_embedding)\n",
        "    upscaling = tf.keras.layers.Reshape((28, 28, 1))(upscaling)\n",
        "\n",
        "    # merge image with class embedding\n",
        "    input_image = tf.keras.layers.Input(shape=(28, 28, 3))\n",
        "    merge = tf.keras.layers.Concatenate()([input_image, upscaling])\n",
        "\n",
        "    # define classification architecture\n",
        "    x = tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')(merge)\n",
        "    x = tf.keras.layers.LeakyReLU()(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "\n",
        "    x = layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    model = tf.keras.Model([input_image, input_label], output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V1zofMn40y0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def generator_loss(predictions):\n",
        "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    return cross_entropy(tf.ones_like(predictions), predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z3WdIEg4ZCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_and_save_images(model, epoch, test_input, test_labels):\n",
        "    predictions = model([test_input, test_labels], training=False)\n",
        "\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(predictions[i, :, :, :] / 2. + 0.5)\n",
        "        # TODO figure out how to upsample mean 0 and unit variance\n",
        "        # plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.xlabel(num_to_char[test_labels[i][0]])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "    plt.close()\n",
        "    try:\n",
        "      files.download('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "    except Exception:\n",
        "      print(Exception)\n",
        "      pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slRA1OBoFtj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def image_norm(img):\n",
        "  return (img - 127.5) / 127.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOW2jrDmnNv7",
        "colab_type": "code",
        "outputId": "b0763053-83f1-4618-b173-1bf8eae3dcf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Get ground truth data\n",
        "data_dir = tf.keras.utils.get_file('English',\n",
        "                                    origin='http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/EnglishImg.tgz',\n",
        "                                    untar=True, extract=True)\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "image_path = os.path.join(data_dir, \"Img/GoodImg/Bmp\")\n",
        "\n",
        "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function= lambda img: (img - 127.5) / 127.5)\n",
        "image_ground_truth = image_generator.flow_from_directory(image_path, target_size=(28, 28),\n",
        "                                                          batch_size=BATCH_SIZE,\n",
        "                                                          class_mode='sparse')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 7705 images belonging to 62 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmtYEq344QHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Model\n",
        "num_classes = 62\n",
        "\n",
        "generator = make_generator_model(n_classes=num_classes)\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator = make_discriminator_model(n_classes=num_classes)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                  discriminator_optimizer=discriminator_optimizer,\n",
        "                                  generator=generator,\n",
        "                                  discriminator=discriminator)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVVyZ1Sb4TO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPyic9uYo5kD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define training procedure\n",
        "@tf.function\n",
        "def train_step(images, ground_truth_labels):\n",
        "    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
        "    random_labels = np.random.randint(0, num_classes, BATCH_SIZE).reshape((-1, 1))\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        fakes = generator([noise, random_labels], training=True)\n",
        "        ground_truth_preds = discriminator([images, ground_truth_labels], training=True)\n",
        "        fake_preds = discriminator([fakes, random_labels], training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_preds)\n",
        "        disc_loss = discriminator_loss(real_output=ground_truth_preds, fake_output=fake_preds)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "def train(dataset, epochs, ckpt_prefix):\n",
        "    print(\"Starting training\")\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "        i = 0\n",
        "        for (image_batch, labels_batch) in dataset:\n",
        "            train_step(image_batch, labels_batch)\n",
        "            i += 1\n",
        "            if i > len(dataset):\n",
        "              break\n",
        "        generate_and_save_images(generator, epoch+1, seed, seed_labels)       \n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            checkpoint.save(file_prefix=ckpt_prefix)\n",
        "        print('Time for epoch {} is {} sec'.format(epoch+1, time.time()-start))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbrs4IFm7xzm",
        "colab_type": "code",
        "outputId": "98f96bfa-3926-4c3a-ee32-1eddc50b0091",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml85TFUa4-Qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = '/content/drive/My Drive/char_gan_ckpts/'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLbSUSv19QRB",
        "colab_type": "code",
        "outputId": "18718f04-2b2f-40bb-fc21-312d652ca36a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "# Setup repeated predictions\n",
        "num_examples_to_generate = 16\n",
        "seed = tf.random.normal([num_examples_to_generate, NOISE_DIM])\n",
        "seed_labels = np.random.randint(0, num_classes, num_examples_to_generate).reshape((-1, 1))\n",
        "fakes = generator([seed, seed_labels], training=False)\n",
        "generate_and_save_images(generator, 0, seed, seed_labels)\n",
        "\n",
        "train(image_ground_truth, epochs=1000, ckpt_prefix=checkpoint_prefix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training\n",
            "Time for epoch 1 is 16.799748420715332 sec\n",
            "Time for epoch 2 is 12.489586353302002 sec\n",
            "Time for epoch 3 is 12.371983051300049 sec\n",
            "Time for epoch 4 is 13.284242630004883 sec\n",
            "Time for epoch 5 is 13.521299362182617 sec\n",
            "Time for epoch 6 is 13.409937858581543 sec\n",
            "Time for epoch 7 is 13.410012006759644 sec\n",
            "Time for epoch 8 is 13.491127729415894 sec\n",
            "Time for epoch 9 is 13.390483140945435 sec\n",
            "Time for epoch 10 is 13.862630605697632 sec\n",
            "Time for epoch 11 is 13.842414855957031 sec\n",
            "Time for epoch 12 is 13.328455448150635 sec\n",
            "Time for epoch 13 is 13.368075370788574 sec\n",
            "Time for epoch 14 is 13.336076736450195 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa5aa3YP_CyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}